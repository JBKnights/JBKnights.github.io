---
title: "WildScenes: A Benchmark for 2D and 3D Semantic Segmentation in Large-scale Natural Environments"
collection: publications
category: manuscripts
permalink: https://journals.sagepub.com/doi/10.1177/02783649241278369
excerpt: This paper introduce WildScenes, a bi-modal benchmark dataset consisting of multiple large-scale, sequential traversals in natural environments, including semantic annotations in high-resolution 2D images and dense 3D LiDAR point clouds, and accurate 6-DoF pose information. The data is (1) trajectory-centric with accurate localization and globally aligned point clouds, (2) calibrated and synchronized to support bi-modal training and inference, and (3) containing different natural environments over 6 months to support research on domain adaptation. Our 3D semantic labels are obtained via an efficient, automated process that transfers the human-annotated 2D labels from multiple views into 3D point cloud sequences, thus circumventing the need for expensive and time-consuming human annotation in 3D. We introduce benchmarks on 2D and 3D semantic segmentation and evaluate a variety of recent deep-learning techniques to demonstrate the challenges in semantic segmentation in natural environments. We propose train-val-test splits for standard benchmarks as well as domain adaptation benchmarks and utilize an automated split generation technique to ensure the balance of class label distributions.
date: 2024-09-20
venue: 'The International Journal of Robotics Research (IJRR)'
bibtexurl: https://scholar.googleusercontent.com/scholar.bib?q=info:Hm8hV6yYQpgJ:scholar.google.com/&output=citation&scisdr=CgIrYr3uEKrk9pmzufE:AAZF9b8AAAAAaFu1ofFEAt17EzfHPVe2FDTsT3U&scisig=AAZF9b8AAAAAaFu1obRKuGJYzbuW83doVgpkMl0&scisf=4&ct=citation&cd=-1&hl=en
websiteurl: https://csiro-robotics.github.io/WildScenes/
githuburl: https://github.com/csiro-robotics/WildScenes
paperurl: https://arxiv.org/pdf/2312.15364
citation: 'Vidanapathirana K, Knights J, Hausler S, et al. <i>WildScenes: A benchmark for 2D and 3D semantic segmentation in large-scale natural environments.</i> The International Journal of Robotics Research. 2024;44(4):532-549. doi:10.1177/02783649241278369'
---

This paper introduce WildScenes, a bi-modal benchmark dataset consisting of multiple large-scale, sequential traversals in natural environments, including semantic annotations in high-resolution 2D images and dense 3D LiDAR point clouds, and accurate 6-DoF pose information. The data is (1) trajectory-centric with accurate localization and globally aligned point clouds, (2) calibrated and synchronized to support bi-modal training and inference, and (3) containing different natural environments over 6 months to support research on domain adaptation. Our 3D semantic labels are obtained via an efficient, automated process that transfers the human-annotated 2D labels from multiple views into 3D point cloud sequences, thus circumventing the need for expensive and time-consuming human annotation in 3D. We introduce benchmarks on 2D and 3D semantic segmentation and evaluate a variety of recent deep-learning techniques to demonstrate the challenges in semantic segmentation in natural environments. We propose train-val-test splits for standard benchmarks as well as domain adaptation benchmarks and utilize an automated split generation technique to ensure the balance of class label distributions.